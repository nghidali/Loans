---
title: "Long Form"
author: "Natalie Ghidali"
date: "3/15/2021"
output:
  html_document:
    toc: true
    toc_float: true
    highlight: "tango"
    code_folding: hide
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Introduction
### Motivation
I began mining bitcoin back in high school. Back then, each bitcoin was only worth a few dollars, and the hashes were crackable just from my laptop. It was an easy way to make money, but I never thought too much of it until the price started going crazy. I have always wondered what is really influencing the price of bitcoin. I have heard several theories, among them is that when the world seems uncertain, people turn to cryptocurrency. I was interested in looking at historical new headlines and seeing how the price of bitcoin responded to different world events and cultural movements. I wanted to use Natural Language Processing to better understand the price of bitcoin.

### Data Sources
I downloaded both datasets from kaggle.
The historical price of bitcoin: \href https://www.kaggle.com/mczielinski/bitcoin-historical-data 
Financial news headlines scraped from  CNBC, the Guardian, and Reuters https://www.kaggle.com/notlucasp/financial-news-headlines?select=guardian_headlines.csv

The historical price of bitcoin csv file has bitcoin exchanges for the time period of Jan 2012 to December 2020, with minute to minute updates of OHLC (Open, High, Low, Close), Volume in BTC and indicated currency, and weighted bitcoin price. 

The financial news headlines dataset consists of three csv files, cnbc headlines, guardian headlines, and reuters headlines. The cnbc and reuters headlines csv files include three columns, a date, headline, and description. The guardian headline csv only contains a date and headline column.

### Research Question
Do news headlines have any predictive power in determining the price of bitcoin? 

# EDA

```{r message=FALSE, warning=FALSE, include=FALSE}
# Load Libraries and Set Seed
library(tidyverse)
library(tidyr)
library(lubridate)
library(dplyr)
library(tidytext)
library(stringr)
library(scales)
library(purrr)
library(broom)
library(janitor)
library(lazyeval)
library(tidymodels)
library(textrecipes)
library(parsnip)
library(textfeatures)

set.seed(42)
```

### Script 1: Initial Data Processing
First we load in the data from the three seperate csv files, clean up the column names, and put the dates in the same format.
```{r, eval = FALSE}
# Clean Bitcoin Headlines
bitcoin_prices <- read_csv("data/unprocessed/bitcoin_prices.csv")
bitcoin_prices <- bitcoin_prices %>% 
  drop_na() %>% 
  clean_names()

#  Headlines available only from December 2017 to July 19th, 2020
bitcoin_prices <- bitcoin_prices %>%
  mutate(time = as_datetime(timestamp)) %>%
  filter(time > dmy("01-12-2017"))
  
saveRDS(bitcoin_prices, "data/processed/bitcoin_prices.rda")

# Clean CNBC Headlines
cnbc_headlines <- read_csv("data/unprocessed/cnbc_headlines.csv")
cnbc_headlines <- cnbc_headlines %>%
  select(-Description) %>%
  drop_na() 

times <- cnbc_headlines$Time
times <- gsub("Sept","Sep",times)
times <- str_remove(times, ".ET")
times <- parse_date_time(times,"%I:%M  %p %a, %d %m %y")
cnbc_headlines$Time <- times
saveRDS(cnbc_headlines, "data/processed/cnbc_headlines.rda")

# Clean Guardian Headlines
guardian_headlines <- read_csv("data/unprocessed/guardian_headlines.csv")
guardian_headlines <- guardian_headlines %>%
  drop_na() %>%
  mutate(new_time = dmy(Time))

bad <- subset(guardian_headlines,is.na(guardian_headlines$new_time)) # these rows have incomplete dates where the year is unavailable, we will drop these rows, for forecasting we need full dates

guardian_headlines <- guardian_headlines %>%
  drop_na() %>%
  select(-Time) %>%
  rename(Time = new_time)

saveRDS(guardian_headlines, "data/processed/guardian_headlines.rda")

# Clean Reuters Headlines
reuters_headlines <- read_csv("data/unprocessed/reuters_headlines.csv")
reuters_headlines <- reuters_headlines %>%
  select(-Description) %>%
  drop_na() %>%
  mutate(Time = mdy(Time))

saveRDS(reuters_headlines, "data/processed/reuters_headlines.rda")
```
```{r}
bitcoin_prices <- readRDS("data/processed/bitcoin_prices.rda")
cnbc_headlines <- readRDS("data/processed/cnbc_headlines.rda")
guardian_headlines <- readRDS("data/processed/guardian_headlines.rda")
reuters_headlines <- readRDS("data/processed/reuters_headlines.rda")
```

### Script 2: Combining headlines into a common dataset:
Since we want to see which events are going to influence the price of bitcoin, we can now combine all of our headlines into one dataset to try to get the most possible events as potential predictors.

```{r}
# Pivot headlines into one common dataset
news <- bind_rows(mutate(cnbc_headlines, website = "cnbc"),
                       mutate(reuters_headlines, website = "reuters"),
                       mutate(guardian_headlines, website = "guardian"))

headlines <- news %>%
  # pivot so each word is timestamped in its own row, keep full headlines so we can interpret for events later
  unnest_tokens(word, Headlines, token = "words", drop = FALSE) %>%
  # implement standard naming convention
  clean_names() %>%
  # remove stop words
  filter(!word %in% stop_words$word,
         !word %in% str_remove_all(stop_words$word, "'"),
         str_detect(word, "[a-z]")) %>%
  arrange(time)

saveRDS(headlines, "data/processed/headlines.rda")
```

### Script 3: Quick look at text data
Use log-odds to see which words are more likely to come from each news source datasets.
```{r message=FALSE, warning=FALSE}
# count how many times each word is used and keep only the words used more than 10 times
word_ratios <- headlines %>%
  count(word, website)

# spread website so each website has its own column with row values equal to the word counts
word_ratios <- word_ratios %>%
  spread(website, n, fill = 0)

# calculates log ratios for each site: (source: https://stackoverflow.com/questions/28973056/in-r-pass-column-name-as-argument-and-use-it-in-function-with-dplyrmutate-a) 
log_odds <- function(word_ratios, news1, news2) {
  word_log_odds <- word_ratios %>%
    mutate_if(is.numeric, list( ~ (. + 1) / (sum(.) + 1))) 

  word_log_odds <- word_log_odds %>%
    mutate_(logratio = interp(~ log(news1 / news2), news1 = as.name(news1), news2 = as.name(news2)))
  
  word_log_odds %>%
    group_by(logratio < 0) %>%
    top_n(15, abs(logratio)) %>%
    ungroup() %>%
    mutate(word = reorder(word, logratio)) %>%
    ggplot(aes(word, logratio, fill = logratio < 0)) +
    geom_col(show.legend = FALSE) +
    coord_flip() +
    ylab(paste("log odds ratio (", news1, "/", news2, ")", sep = "")) +
    scale_fill_discrete(name = "", labels = c(news1, news2))
}

log_odds(word_ratios,"reuters","cnbc")
log_odds(word_ratios,"reuters","guardian")
log_odds(word_ratios,"cnbc","guardian")
```

We can see that Reuters talks more about foreign policy, while CNBC talks more about individual companies like "nvidia" and "chipotle" and someone named Jim Cramer a lot. After some quick googling, I see that this is a CNBC correspondant responsible for covering all things money. I'll keep him in in case his opinions hold sway over the bitcoin market. The guardian seems to cover a lot of British news, with words like "tory" and "britains". After some googling, I see that William Keegan is their senior economics commentator, and Nils Pratley is their financial editor, so "keegan" and "pratley" also make sense. 

### Script 4: Analyzing the distribution of headlines posted time
The first thing to note is that we have multiple options for outcome variables here. We could potentially try to predict Open, High, Low, Close, or Weighted Price. Lets see what times most news headlines are posted at. We unfortunately are missing the time information for Reuters and the Guardian, they are not available from the dataset. We only have that information for CNBC.
```{r, message = FALSE}
headlines %>%
  # define a new time variable in the data frame that defines which unit of time each tweet was posted in
  mutate(hour = hour(time)) %>%
  filter(website == "cnbc") %>%
  ggplot(aes(hour)) +
  geom_histogram(position = "dodge") +
  scale_x_continuous(breaks = seq(0,24,1))
```

From the above, we can see that most CNBC headlines are posted later in the day, between 6pm and 9pm. For Reuters and the Guardian, from a quick perusal of their website it seems most market articles are posted in the afternoon or evening. 

### Script 5: Check on what day of the week most headlines are posted.
```{r, message=FALSE}
headlines %>%
  # define a new time variable in the data frame that defines which unit of time each tweet was posted in
  mutate(wday = wday(time)) %>%
  ggplot(aes(wday), color = website, fill=website) +
  geom_histogram(position = "dodge") +
  scale_x_continuous(breaks = 1:7, labels = c("Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"), name = "weekday")
```

Here we can see that fewer headlines are posted on the weekend then on weekdays.

### Script 6: Check for missingness issues
```{r}
bitcoin_prices <- readRDS("data/processed/bitcoin_prices.rda")
headlines <- readRDS("data/processed/headlines.rda")

skimr::skim(bitcoin_prices)
skimr::skim(headlines)
```
No data is missing. 

### Script 7: Price of bitcoin over time
For the price of bitcoin, I am going to choose to predict close. The close price will reflect the events that people heard about that day. Let's look at what the price of bitcoin has done over time.
```{r}
bitcoin_prices <- readRDS("data/processed/bitcoin_prices.rda")
bitcoin_prices %>%
  ggplot(aes(time,close)) +
  geom_point() +
  scale_x_datetime(date_breaks = "1 year",date_labels = "%Y")
```

We can see that while we have many ups and downs, the price of bitcoin has generally increased. What we really want is to predict if the price of bitcoin will move up or down on a given day based on the events that happened that day, so we need to create a new column that represents the percent increase in price from the previous day. We will also have to narrow down our dataset and pick a specific time to look at the price of bitcoin each day. Lets check on how the price of bitcoin changes over the course of a given day

### Script 8: How the price of bitcoin changes over the course of a day
```{r, message=FALSE, warning=FALSE}
hourly_bitcoin_prices <- bitcoin_prices %>%
  select(time, close) %>%
  mutate(floored_time = floor_date(time, unit = "hours")) %>%
  group_by(floored_time) %>%
  summarise(hourly_close = mean(close)) 

hourly_bitcoin_prices <- hourly_bitcoin_prices %>%
  mutate(pct_change = ((hourly_close - lag(hourly_close)) / lag(hourly_close)) * 100)

hourly_bitcoin_prices %>%
  mutate(hour = hour(floored_time)) %>%
  group_by(hour) %>%
  summarise(average_hourly_pct_change = mean(pct_change)) %>%
  ggplot(aes(hour, average_hourly_pct_change)) +
    geom_point() +
  geom_smooth(se = FALSE) +
  labs(y = "Average percent change from previous hour",
       title = "The average change in the price of bitcoin over the course of the day")
```

The trend in the change in the price of bitcoin being related to the time of day is surprising to me. I would not have expected to see any pattern here.

### Script 9: Combining bitcoin price data and headlines
I will need to bin my headline data and my bitcoin data by day.
```{r, message = FALSE}
bitcoin_prices <- readRDS("data/processed/bitcoin_prices.rda")
cnbc_headlines <- readRDS("data/processed/cnbc_headlines.rda")
guardian_headlines <- readRDS("data/processed/guardian_headlines.rda")
reuters_headlines <- readRDS("data/processed/reuters_headlines.rda")

# Pivot headlines into one common dataset
news <- bind_rows(mutate(cnbc_headlines, website = "cnbc"),
                       mutate(reuters_headlines, website = "reuters"),
                       mutate(guardian_headlines, website = "guardian"))

# Extract average price by day for bitcoin
daily_bitcoin_prices <- bitcoin_prices %>%
  # Only use the closing price
  select(time, close) %>%
  mutate(date = floor_date(time, unit = "days")) %>%
  # Get the average close price for each day
  group_by(date) %>%
  summarise(close = mean(close)) %>%
  # Calculate the percent change in closing price from the previous day
  mutate(pct_change = ((close - lag(close)) / lag(close)) * 100) %>%
  # remove first day since we do not have a percent change from the previous day available
  drop_na()

# Extract date from headline time column so it is in same format as bitcoin data
news <- news %>%
  clean_names() %>%
  mutate(date = floor_date(time, unit = "days")) %>%
  select(-time)

bitcoin_news_data <- inner_join(daily_bitcoin_prices, news, by = "date") %>%
  arrange(date) %>%
  select(date,pct_change,headlines)

saveRDS(bitcoin_news_data, "data/processed/bitcoin_news_data.rda")
```

My plan for splitting will be to use initial_time_split with prop = 0.8. I will stratify by percent change in daily closing price.

# Model Fitting
To cut down on the data (it was taking 20 + hours to train) I am only going to analyze days where the percent change in the price of bitcoin was more than 1 standard deviation (as opposed to taking a random subset). This works for my data because I am interested in events that are causing big swings in the price of bitcoin. I don't care as much about days where the price barely moved. 
```{r}
# Preprocessing
bitcoin_news_data <- bitcoin_news_data %>%
  filter(abs(pct_change) > sd(pct_change))
```

```{r}
# Split the data
bitcoin_news_split <- initial_time_split(bitcoin_news_data, prop = 0.8, strata = close)
training_data <- training(bitcoin_news_split)
testing_data <- testing(bitcoin_news_split)
```

I specified a recipe to take the headlines, tokenize them, extract textfeatures, remove stop words, stem words and then hash them for text processing. 
```{r}
# Make a recipe
binary_hash <- function(x) {
  x <- ifelse(x < 0, -1, x)
  x <- ifelse(x > 0,  1, x)
  x
}

basics <- names(textfeatures:::count_functions)

bitcoin_recipe <-
  recipe(pct_change ~ headlines + date, data = training_data) %>%
  # Do not use the date as a predictor
  update_role(date, new_role = "id") %>%
  update_role(headlines, new_role = "predictor") %>%
  # Make a copy of the raw text
  step_mutate(headlines_raw = headlines) %>%
  # Compute the initial features. This removes the `review_raw` column
  step_textfeature(headlines_raw) %>%
  # Make the feature names shorter
  step_rename_at(
    starts_with("textfeature_"),
    fn = ~ gsub("textfeature_headlines_raw_", "", .)
  ) %>%
  # The text is tokenized (i.e. broken into smaller components such as words)
  step_tokenize(headlines)  %>%
  # Stop words (such as “the”, “an”, etc.) are removed
  step_stopwords(headlines) %>%
  # Tokens are stemmed to a common root where possible
  step_stem(headlines) %>%
  # Creates a specification of a recipe step that will convert a tokenlist into multiple variables using the hashing trick.
  step_texthash(headlines) %>%
  # Simplify these names
  step_rename_at(starts_with("headlines_hash"), fn = ~ gsub("headlines_", "", .)) %>%
  # Convert the features from counts to values of -1, 0, or 1
  step_mutate_at(starts_with("hash"), fn = binary_hash) %>%
  # Transform the initial feature set
  # Optionally transform non-token features (the count-based features like number of lowercase characters) to a more symmetric state using a Yeo-Johnson transformation
  step_YeoJohnson(one_of(!!basics)) %>%
  # predictors with a single distinct value are removed
  step_zv(all_predictors()) %>%
  # All predictors are centered and scaled.
  step_normalize(all_predictors()) 
```

Then I fed that recipe into three different models for tuning. I tried random forest, boosted tree, and nearest neighbor. Since computation time on the dataset was a big limitation, I tried to limit the number of variables for tuning. For the Random Forest I tuned mtry and min_n, for the boosted tree I tuned mtry and learn rate, and for nearest neighbor I tuned neighbors.
```{r}
# Set up models
# Tuning with large grid is prohibitively slow, > 2 days and did not finish so we only tune 2 things per model, num_terms, and mtry/neighbors

# A Random Forest Model
rf_model <- rand_forest(mtry = tune(),
                        min_n = tune(),
                        mode = "regression") %>%
  set_engine("ranger")

# A Boosted Tree Model
bt_model <-
  boost_tree(
    mode = "regression",
    mtry = tune(),
    learn_rate = tune()) %>%
  set_engine("xgboost")

# A Nearest Neighbors Model
nn_model <-
  nearest_neighbor(mode = "regression", neighbors = tune()) %>%
  set_engine("kknn")
```

```{r}
# Set up workflows
# A Random Forest Model
rf_workflow <- workflow() %>%
  add_recipe(bitcoin_recipe) %>%
  add_model(rf_model)

# A Boosted Tree Model
bt_workflow <- workflow() %>%
  add_recipe(bitcoin_recipe) %>%
  add_model(bt_model)

# A Nearest Neighbors Model
nn_workflow <- workflow() %>%
  add_recipe(bitcoin_recipe) %>%
  add_model(nn_model)
```

Since the random forest was taking the longest to train, I used a 3x3 grid. For the boosted tree and nearest neighbor models I was able to use a 5x5 grid for tuning.

```{r}
# Set up grids
# Random forest model
rf_params <- parameters(rf_workflow) %>%
  update(mtry = mtry(range = c(1, 295))) 
rf_grid <- grid_regular(rf_params, levels = 3)

# Boosted Tree model
bt_params <- parameters(bt_workflow) %>%
  update(mtry = mtry(range = c(1, 295))) %>%
  update(learn_rate = learn_rate(range = c(-5,1)))
bt_grid <- grid_regular(bt_params, levels = 5)

# Nearest neighbors model
nn_params <- parameters(nn_workflow) 
nn_grid <- grid_regular(nn_params, levels = 5) 
```

I used 10 fold cross validation for tuning.
```{r}
# Set up globals
# Due to training time, only repeat once, but for validation use 10 folds (dataset is huge)
folds <- vfold_cv(training_data, v = 10, repeats = 1, strata = pct_change)
control <- control_resamples(verbose = TRUE)
```

```{r}
# Write out necessary objects for jobs
save(rf_workflow, rf_grid, folds, control, file = "data/model/rf_variables.rda")
save(bt_workflow, bt_grid, folds, control, file = "data/model/bt_variables.rda")
save(nn_workflow, nn_grid, folds, control, file = "data/model/nn_variables.rda")
```

```{r, eval = FALSE}
# Random Forest Tuning Script: rf_tuned.R
load("data/model/rf_variables.rda")

rf_tuned <- rf_workflow %>%
  tune_grid(folds, grid = rf_grid, control = control)

saveRDS(rf_tuned, "data/model/rf_tuned_fit.rda")
```

```{r, eval = FALSE}
# Boosted Tree Tuning Script: bt_tuned.R
load("data/model/bt_variables.rda")

bt_tuned <- bt_workflow %>%
  tune_grid(folds, grid = bt_grid, control = control)

saveRDS(bt_tuned, "data/model/bt_tuned_fit.rda")
```

```{r, eval = FALSE}
# Nearest Neighbor Tuning Script: nn_tuned.R
load("data/model/nn_variables.rda")

nn_tuned <- nn_workflow %>%
  tune_grid(folds, grid = nn_grid, control = control)

saveRDS(nn_tuned, "data/model/nn_tuned_fit.rda")
```

```{r}
# Load data from completed jobs
rf_tuned_fit <- readRDS("data/model/rf_tuned_fit.rda")
bt_tuned_fit <- readRDS("data/model/bt_tuned_fit.rda")
nn_tuned_fit <- readRDS("data/model/nn_tuned_fit.rda")
```

### Analyze Models

Random Forest
```{r}
autoplot(rf_tuned_fit, metric = "rmse") # Random Forest
```

For the Random Forest fewer predictors performs better. Node size has a negligible impact on RMSE.

Boosted Tree
```{r}
autoplot(bt_tuned_fit, metric = "rmse") # Boosted Tree
```

For the Boosted Tree a learning rate of 0.1 performed optimally, the number of randomly selected predictors had a negligible impact on RMSE.

Nearest Neighbor
```{r}
autoplot(nn_tuned_fit, metric = "rmse") # Nearest Neighbor
```

For the nearest neighbor model increasing the neighbors improved RMSE, however these improvements begin to level off as we keep adding neighbors.

### Pick the best model
Random Forest Models Tuned Performance
```{r}
show_best(rf_tuned_fit, metric = "rmse")
# rmse = 5.891875
```

Boosted Tree Models Tuned Performance
```{r}
show_best(bt_tuned_fit, metric = "rmse")
# rmse = 5.894736
```

Nearest Neighbors Models Tuned Performance
```{r}
show_best(nn_tuned_fit, metric = "rmse")
# rmse = 6.104773	
```
Boosted tree and random forest both performed well with RMSE of ~5.89. Since boosted tree was significantly faster to train, I will pick the boosted tree.


### Finalize Model
```{r}
bt_results <- bt_workflow %>% 
  finalize_workflow(select_best(bt_tuned_fit, metric = "rmse")) %>%
  fit(training_data)

bt_results[2]
```

We can see that after 10 iterations the performance of the tuned boosted tree with mtry of 221 and learn rate of 0.1 is 5.82.

# Model Performance on Testing Set

```{r}
bitcoin_metrics <- metric_set(rmse)
bt_predictions <- predict(bt_results, new_data = testing_data) %>%
  bind_cols(testing_data %>% select(pct_change)) 

bt_predictions %>%
  bitcoin_metrics(truth = pct_change, estimate = .pred)
```

On the testing set the best model has an RMSE of 9.362324. This is significantly higher than the models performance on the training set. This would suggest to me that there may be some overfitting.

```{r}
bt_predictions %>%
  arrange(pct_change) %>%
  ggplot(aes(pct_change, .pred)) +
    geom_point() +
  labs(x = "Observed Percent Change in Price of Bitcoin",
       y = "Predicted Percent Change in Price of Bitcoin") 
```

From the above plot it seems that our model is performing poorly pretty much uniformly across the testing set.

# Debrief and Next Steps

The first and easiest next step would be to use the entire dataset including the middle standard deviation. Working on a subset of the data was necessary for this project due to computational constraints. However, if more time was available training the model on the full dataset could help find patterns. Another step would be to add in more fine tuning. For instance, the num_terms in the hashing step function can be tuned. This was omitted due to time constraints in this report but could aid in finding predictive patterns. Another possibility would be to train a neural network on this dataset. Neural nets have been used successfully on Natural Language Processing models by other data scientists. It could also be interesting to take more granularity in the observed time the headline was posted and a resulting change in the price of bitcoin. For example, seeing if there is any merit in adding a 1 day delay in percent change in bitcoin price to account for reaction time to a given headline.
